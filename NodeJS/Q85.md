# ðŸ§© Scenario: Large File Upload with Pre-signed URLs (Client + Server + Cloud Storage)

---

## Why Pre-signed URLs?

* Client uploads **directly** to cloud storage (no server memory overhead)
* Server generates a **temporary, secure URL** for upload
* Server stores file **metadata** for later use

---

# 1ï¸âƒ£ Server Side: Generate Pre-signed URL (Node.js + AWS SDK)

```js
const express = require('express');
const AWS = require('aws-sdk');
const app = express();
const s3 = new AWS.S3({
  region: 'us-east-1', // your bucket region
});

app.get('/generate-upload-url', (req, res) => {
  const { filename, filetype } = req.query;
  const params = {
    Bucket: 'my-bucket-name',
    Key: `uploads/${Date.now()}-${filename}`, // unique key for each upload
    Expires: 60, // URL valid for 60 seconds
    ContentType: filetype,
  };

  try {
    const uploadUrl = s3.getSignedUrl('putObject', params);
    res.json({ uploadUrl });
  } catch (err) {
    console.error(err);
    res.status(500).json({ error: 'Could not generate upload URL' });
  }
});

app.listen(3000, () => {
  console.log('Upload server running on port 3000');
});
```

---

# 2ï¸âƒ£ Client Side: Upload Large File Using Pre-signed URL (React + fetch)

```jsx
import React, { useState } from 'react';

function LargeFileUpload() {
  const [file, setFile] = useState(null);
  const [uploadStatus, setUploadStatus] = useState('');

  const handleFileChange = (e) => {
    setFile(e.target.files[0]);
  };

  const uploadFile = async () => {
    if (!file) return alert('Please select a file');

    // 1. Get pre-signed URL from your server
    const response = await fetch(
      `/generate-upload-url?filename=${encodeURIComponent(file.name)}&filetype=${encodeURIComponent(file.type)}`
    );
    const { uploadUrl } = await response.json();

    setUploadStatus('Uploading...');

    // 2. Upload file directly to S3 using fetch PUT
    const uploadResponse = await fetch(uploadUrl, {
      method: 'PUT',
      headers: {
        'Content-Type': file.type,
      },
      body: file,
    });

    if (uploadResponse.ok) {
      setUploadStatus('Upload successful!');
    } else {
      setUploadStatus('Upload failed.');
    }
  };

  return (
    <div>
      <input type="file" onChange={handleFileChange} />
      <button onClick={uploadFile}>Upload Large File</button>
      <p>{uploadStatus}</p>
    </div>
  );
}

export default LargeFileUpload;
```

---

# 3ï¸âƒ£ How Does the Server Handle This?

* The server **only generates a short-lived URL** with permissions to upload a single object to S3
* The client then uploads the **file contents directly to S3** (using the pre-signed URL PUT request)
* This means **no file data flows through your server**, avoiding memory or bandwidth bottlenecks
* Your server remains **stateless and scalable**

---

# 4ï¸âƒ£ Storing File Metadata (Optional)

After the upload completes, you can:

* Send a request from client â†’ server with the file metadata (S3 object key, size, user info)
* Server stores metadata in DB for tracking or later retrieval

Example (server API to save metadata):

```js
app.post('/save-file-metadata', express.json(), (req, res) => {
  const { key, size, userId } = req.body;
  // Save to DB (pseudo code)
  db.files.insert({ key, size, userId, uploadedAt: new Date() });
  res.status(200).send('Metadata saved');
});
```

---

# ðŸš€ Summary

| Step | Who    | What Happens                                                |
| ---- | ------ | ----------------------------------------------------------- |
| 1    | Client | Requests upload URL from server                             |
| 2    | Server | Generates pre-signed URL from S3 with PUT permissions       |
| 3    | Client | Uploads file directly to S3 using pre-signed URL (streamed) |
| 4    | Client | Optionally notifies server about upload metadata            |
| 5    | Server | Stores metadata in DB for future reference                  |

---

# ðŸ”¥ Why This Pattern?

* Efficient for **large files** (1GB+)
* Server **never buffers file** â†’ no memory overhead
* Scales well with many concurrent uploads
* Keeps server code clean and focused on business logic