## âœ… What is Stream Chaining in Node.js?

> **Stream chaining** is the process of **connecting multiple stream operations together** using `.pipe()` to perform complex data processing in a **memory-efficient** way.

This is common in tasks like:

* Reading a file
* Compressing it
* Encrypting or modifying it
* Writing it to another file or sending to a client

---

## ðŸ“¦ Real-World Example: Compress a File

Let's say you want to:

1. Read a large `.txt` file
2. Compress it using `gzip`
3. Save the compressed output to a new file

You can **chain streams** like this:

```js
const fs = require('fs');
const zlib = require('zlib');

// Chain: read file â†’ compress â†’ write to new file
fs.createReadStream('bigfile.txt')
  .pipe(zlib.createGzip())         // transform stream (gzip)
  .pipe(fs.createWriteStream('bigfile.txt.gz'))  // writable stream
  .on('finish', () => {
    console.log('File successfully compressed!');
  });
```

âœ… This processes the file **chunk by chunk** â€” never loading the entire file into memory!

---

## ðŸ§  Why Use Stream Chaining?

| Feature            | Benefit                                   |
| ------------------ | ----------------------------------------- |
| âœ… Memory-efficient | Doesnâ€™t load full file/data into memory   |
| âœ… Fast             | Uses internal buffering and backpressure  |
| âœ… Clean syntax     | Chaining makes complex pipelines readable |

---

## ðŸ§  Interview One-Liner:

> "Stream chaining in Node.js is when multiple stream operations are linked using `.pipe()`, allowing for efficient, step-by-step processing of large data â€” like reading, transforming, and writing files without loading them entirely into memory."