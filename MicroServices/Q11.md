# üõ†Ô∏è How to Troubleshoot Microservices (Node.js + Tools)

## üéØ Step-by-Step Troubleshooting Strategy

---

### ‚úÖ 1. **Start with the Symptom**

> Is it a crash, error, timeout, 500, latency, or bad data?

Ask yourself:

* Which service is showing the issue?
* Is the issue client-side or server-side?
* Is it affecting all users or a subset?

---

### ‚úÖ 2. **Check Logs First (Node.js)**

Each microservice should have **structured logs**.

### ü™µ Winston Logging (Example):

```js
const winston = require('winston');

const logger = winston.createLogger({
  transports: [new winston.transports.Console()],
});

logger.info('User created', { userId: 123 });
logger.error('Payment failed', { orderId: 456, error: err });
```

üëâ Centralize logs using:

* üì¶ **ELK Stack** (Elasticsearch, Logstash, Kibana)
* üì¶ **Grafana Loki**
* üì¶ **Datadog**, **CloudWatch**, **Loggly**

---

### ‚úÖ 3. **Use Distributed Tracing**

Helps you **follow a request across services**.

üì¶ Tools:

* **Jaeger** (open-source)
* **Zipkin**
* **OpenTelemetry**

### Example with OpenTelemetry + Node.js

```js
const { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node');
const { registerInstrumentations } = require('@opentelemetry/instrumentation');

const provider = new NodeTracerProvider();
provider.register();
```

üëâ Attach a trace ID to logs for better debugging.

---

### ‚úÖ 4. **Check Metrics**

> Are CPU, memory, DB queries, or response times high?

üìà Use Prometheus + Grafana to track:

* API latency
* Error rates (5xx)
* DB query time
* Queue backlog

### Node.js Example (Prometheus client):

```js
const client = require('prom-client');

const httpRequestDurationMicroseconds = new client.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests',
  labelNames: ['method', 'route', 'status_code'],
});
```

---

### ‚úÖ 5. **Investigate Inter-Service Failures**

If a service fails, it could be caused by **another service**.

Use:

* API Gateway logs (e.g. NGINX)
* Circuit breaker logs
* Retry patterns

### Axios Retry Example:

```js
const axiosRetry = require('axios-retry');
axiosRetry(axios, { retries: 3, retryDelay: () => 1000 });
```

---

### ‚úÖ 6. **Check Message Queues (If Used)**

* Are messages stuck in RabbitMQ/Kafka?
* Are consumers slow or crashing?

Use:

* RabbitMQ Admin
* Kafka Manager / Confluent UI
* Dead-letter queues (DLQ)

---

### ‚úÖ 7. **Reproduce Locally or in Staging**

> Always try to replicate the issue in a **safe environment**.

Use Docker Compose:

```bash
docker-compose up user-service order-service gateway
```

Add `.env` flags for **debug mode** and more verbose logs.

---

### ‚úÖ 8. **Database & Cache Checks**

* Long-running queries?
* Index missing?
* Redis cache expired?

üì¶ Tools:

* `pg_stat_activity` (Postgres)
* `EXPLAIN` queries
* Redis CLI: `monitor`, `keys`, `get`

---

### ‚úÖ 9. **Health Checks & Liveness Probes**

Make sure each service exposes a health endpoint:

```js
app.get('/health', (req, res) => {
  res.status(200).send('OK');
});
```

Kubernetes will use this to auto-restart bad pods.

---

## üîÅ Quick Cheat Sheet: Microservices Troubleshooting

| What to Check      | Tool / Method                 |
| ------------------ | ----------------------------- |
| üîç Logs            | Winston, ELK, Loki            |
| üßµ Trace           | Jaeger, OpenTelemetry         |
| üìä Metrics         | Prometheus + Grafana          |
| üîå API errors      | API Gateway logs, retry logic |
| üì® Message delays  | RabbitMQ, Kafka monitoring    |
| ‚öôÔ∏è Performance     | APM tools (Datadog, NewRelic) |
| üõ† DB/Cache issues | Redis CLI, Postgres EXPLAIN   |
| ‚úÖ Health status    | `/health` endpoint            |
| üß™ Reproduce issue | Docker Compose, staging tests |

---

## üí¨ Interview-Ready Answer

> ‚ÄúTo troubleshoot microservices, I follow logs and traces end-to-end using tools like OpenTelemetry and centralized loggers. I monitor key metrics via Prometheus and Grafana, and if queues or DBs are involved, I inspect queue backlogs or query performance. I also make sure services expose health checks and reproduce the issue in a safe environment like staging.‚Äù